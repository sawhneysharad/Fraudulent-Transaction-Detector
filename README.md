# Fraudulent-Transaction-Detector

# Data Source- UCI Machine Learning Repository
http://archive.ics.uci.edu/ml/index.php

# Introduction

In this project, we are discussing about different classification algorithms for analyzing the real time dataset and comparing the performance metrics between the different algorithms. 
We also finding the best fit model by evaluating the different algorithm based on business problem.
In this project, we are identifying fraudulent transaction based on features from synthetic financial dataset generated by the PAYSIM mobile money simulator.
We have followed below steps:
1.	Business Understanding - Project objectives and Requirement Understanding 
2.	Exploring & Understanding Data – initial data collection & familiarization 
3.	Data Preparation – data cleaning, Exploratory data analysis, data transformation 
4.	Modelling - selecting model, Build/ Training Model, Prediction  
•	KNN Algorithms
•	Navies Bayes 
•	Decision Tree
•	Random Forest 
•	Logistic Regression 
5.	Model Evaluation – model evaluation, review results, finding the best fit model
6.	Deployment – Deploying the best fit model based on success criteria.
By providing the effective detection of fraudulent transaction will helpful in saving millions of dollars from finical institutions and customers. 
# Analysis
1.	Business Problem:
With advancement in technology, finical institutions are rapidly adopting digital payments, these experience rapid growth in their traction volumes. along with digital transformation, there is exponential growth in fraudulent transactions in these payments. Effective fraud detection system should be able to detect the fraudulent transaction effectively from doing online frauds and same time not preventing genuine users for doing online payments.  
Converting Business objective into Analytical Objective:
Our objective is to classify the fraud and non-fraud transaction by creating the class boundary in the feature space defined by input features i.e. transaction details. Each transaction can be represented as vector of its feature values. Using the different classification algorithms and comparing them.
2.	Exploring & Understanding Data: 
PAYSIM simulator finical data set consists of Both Numerical and categorical features like Transaction type, Amount Transferred from origin & destination, time and etc.
Data set contains 6362620 Rows of data about 11 features and our target variable is “ISFRAUD”.
3.	Data Preparation & Exploratory Data Analysis:
•	Handling Missing values: data set don’t have any missing values.
•	Handling Outliers: we didn’t observer any outliers in dataset.
•	Exploratory data analysis:
I.	No of Transactions (b/w fraud & non fraud)
 
From the above graph, we observed only 8213 are fraud out of 6362620 transactions i.e. 0.13% of the total transaction. We observed class imbalance between in our target variable.
II.	Transaction as per type
 
From the above graph, we observed most of transaction types are Cash_out, Payment followed by Cash_in, Transfer. And very few transactions from the debit in our data set.
III.	 No. of fraud transactions based on type 
 
From the above graph, it is clear that most of the fraud transaction either “type” -“Cash_out” or “Transfer” and zero transaction from the other” type”.
IV.	Fraud transactions amount distribution:
 
The frequency distribution of the amounts from the fraud truncation are positively skewed towards the zero, means most of the fraud transaction amount is lesser amounts.
•	Data Transformation and creating new features
As per above graphs we observed the class bias (No of Transactions (b/w fraud & non fraud Graph), to solve this problem, we need take the sample of records approximately equal proportion from both classes to get the better models.
To eliminate class bias, I took the sample of equal proportion from both classes randomly by using the code. We may not able to use entire data set by taking sample.
We observed the all the fraud transactions are belong to either type “CASH_OUT” or “TRANSFER”, we filtered and kept data that belongs to these two types.
We have created new features for “adjustedBalanceOrg”, and “adjustedBalanceDest” based on calculations.
•	Creating Training and testing data sets: 
By creating test and train data sets from the original data set, Model can learn using train data seta and test data will help to estimate predictive accuracy of the model. We randomly split the data 70 % for training data seta and 30% test dataset.
4.	Model Building:
1.	KNN Classification Algorithm:
KNN is simplest algorithm used for both classification and Regression. Its instance-based (training data for prediction), lazy (no learning of the model required. Instead, it merely stores the training data verbatim) and non- parametric (No assumption about data distribution) model.  Predictions for new test instance (xt) by searching through entire training set for K most similar instances or neighbors and aggerating the output variable for those k instances. For classification this might be the most occurred class vale and for regression it might be the average of output value. In KNN algorithm distance, size of data set and ‘K’ value will decide the performance of the model
Training model:
Scaling numeric variables using z-score transformation: By using the z score we transformed numeric variables using z -score in data set.
Actually, there is no training phase involves in model building for kNN algorithm. It actually stores the training data in structured format. In this algorithm, it is using Euclidian distance of 81 instances of test data to training data and assigning the majority class to output.
 
True Negatives: 832 
, True Positives: 1972
 False Negative: 7 Actually those are fraud predicted as not fraud
False Positives: 4 Actually those are not fraud predicted as fraud
For building model, we need test and train data along predefined k-value in R. we are using ideal value of k i.e. 81 it is square root of size of training data set. Based on above values we build model as below.
There is no standard method for selecting the k-value, but it should balance between underfitting and overfitting and have more accuracy in prediction.
We built the model between different model with different k-values to compare the model accuracy.

K Value	False Negatives	False Positives	Misclassification Error
1	1	3	0.14%
20	1	4	0.21%
50	7	3	0.36%
81	7	4	0.39%
86	7	5	0.43%


Using kNN algorithm is easy for classification and Training time is very less. But it requires more timing for preprocess the data for model where it requires all variables should be numeric and require large amount of memory. We can choose different k values based on training data set size and based performance of the model.
We built the model the model by using normalized features and with different k values found the accuracy of the model 99% for k =81.
2.	Navies Bayes Algorithm:
Navies Bayes simplest and probabilistic classification algorithm. Means they use past data to extrapolate future events or uncertainty in events. Naive Bayes uses the principle of probability for classification, which uses the data about past events to estimate the probability of future events. This classifier uses the Bayesian methods of conditional probability utilizes training data to calculate the probability of class based on features in the data set and later classifier used on unlabeled data, it uses the prior probabilities (Max) to predict the most likely class for the new feature.
In mathematical way, it uses likelihood, prior probability, marginal prior probability to calculate the posterior probability.
we need to transformations like discretize the numeric features by creating bins to calculate frequency tables. 
Navies Bayesian is simple, fast and effective algorithm, it’s easy to obtain the estimated probability for a prediction. This algorithm used for sentiment analysis, recommendation systems, spam analysis, and in different real time application.
Training model:
By using ‘naiveBayes’ function, we can train the model using the train data set.
  
True positives: 1062 predicted the fraud transactions as fraud 
True Negatives: 815 Predicted non fraud transaction as fraud
False negatives: 914 Fraud Transaction as non-fraud transaction. This is very important because classifying as fraud as non- Fraud is huge loss to banks.
False Positives: 24 classifying the non-fraud as fraud, we might lose the customers and reputation of the bank by classifying genuine customers as fraud.
To evaluate the classifier, we need to test its prediction (NBclassfier) on the unlabeled data i.e. transactions_testBT (test data). Now trained model is used to predict the target class of the test data using ‘predict’ function
 
Using Navies Bayesian algorithm very easy to use and to implement the model. With less effort we are able to build classification algorithm to predict the wheatear the Transaction is Fraud or NOT using Naive Bayesian algorithm. Only drawback is Bayesian algorithm treats all the features are independent and equal important. 
We built the model using the Navies Bayes algorithm, after discretizing the numeric features observed the accuracy of 66% with Laplace estimator. 
3.	Decision Tree
Decision tree mostly widely used machine learning algorithms for classification. Decision tree learners build a model in the form tree structure and uses divide and conquer rule to divide data into smaller and smaller subsets of similar classes. The model is like flow chart comprises of logical decisions, Data that to be classified begin at the root node that indicate a decision to made on an attribute. where it goes through branches that indicate the decision’s choices. The path that takes funnels each record into a leaf node, which is result of following combinations of decision (assigns predicted class)
we can build number of trees based on given set of attributes. While some trees are accurate compared to other trees. Finding the best optimal tree is in hypothesis space is computationally infeasible. These algorithms use greedy strategy by selecting locally optimized decisions like which attribute to use partition the data. Which is used many decision tree induction algorithms’ like ID3, C4.5, and CART algorithms.
Training model:
we are using C5.0 algorithm to build the model. Out target variable “default” and Remining variables are predictor variables.
About C5.0: 
a.	Gives binary tree or multi branches tree 
b.	Uses the information gain or entropy for splitting criteria 
c.	Uses pruning technique(boosting) in way reducing the size of tree without loss of model accuracy.
We built the model using C5.0 algorithm
 
We can interpret above tree as, 
i.	“adjustedBalnceOrg” feature is used as root node and split into four nodes based on different levels in that feature.
ii.	For the first node, where condition “NewbalanceOrig < = 0 “is 1. Where it classified the as “Fraud” class (leaf node). 
iii.	For the first node, where condition “NewbalanceOrig < = 5131896 “is 0. Where it classified the as non-Fraud class (leaf node). In that leaf node, 203 indicates 203 reaching the decision and 1 incorrectly classified as nonfraud i.e. 1 transaction are actually Fraud in spite of model’s prediction
Now we are applying decision tree to test data set using predict function and also calculating the model performance using the ‘CrossTable’ function.
 
From we observed the,
False negatives are 5 and False positive are 3.
We built the model using the decision tree for classifying the transaction into “Fraud” & Non-Fraud Transactions and achieved accuracy of 99.5 percentage.
Decision tree classification using C5.0 algorithm is very easy to use, understand and interpret results. It can handle both numeric & nominal feature, missing data and same time features with many levels and large data sets, biased towards splits having a large number of levels and also overfit or underfit the model, if tree grows indefinitely or  try split smaller and smaller partition to classify perfectly in training data set
4.	Random Forest Algorithm
Random Forests are an ensemble defined on decision trees. Random forest is an ensemble classifier that uses multiple models of several decision trees to obtain a better prediction performance and control overfitting.
In the Random Forest model, 
•	Data set is sampled with replacement to generate subsets of training data sets of data i.e. known as bootstrap Samples.
•	Each of these bootstrap samples used to train decision trees separately.
•	Aggerating the of decision trees called Random for ensemble.
•	Result of the ensemble model is calculated based majority vote from all decision trees (Bagging or Bootstrap aggerating)
This model uses two key concepts that give it the name random:
	A random sampling of training data points when building trees.
	Random subsets of features considered when splitting nodes.
Training model:
We built the model using the Random Forest as follows with parameters
Target variable: IsFraud 
Predictor variables: All except “IsFraud ” variable 
mtry: we are specifying number of variables randomly selected for each split.in this model we are using ‘2’
ntree: 20 number of trees
 
In random forest, we will two different measures to find importance of the features.
1.	Mean decrease accuracy: by permuting values in feature and how much permutation decreases the model accuracy (calculated using OOB error) 
2.	  Mean Decrease Impurity: the total decrease in node impurities from splitting on the Feature, averaged over all trees (calculated using Gini Index)
From the importance function, we found that features like “AdjustedBalanceorg”, “oldBalanceorg”, “newbalanceorg” features are important than the rest of the features.
 
Random Forest is providing high accuracy compared to decision trees by reducing the variance (overfitting problem) while maintain the low bias (no need to prune the trees). implementation of the model and interpretation of the results is easy compared to other models.
By using the random forest algorithm, we achieved the accuracy of 99.6 percentage.
5.	Logistic Regression Algorithm:

Logistic Regression, or, Logit Regression is Probabilistic Statistical Classification model, used for predicting the outcome of a categorical dependent variable (i.e. binary class label) based on one or more independent variables (i.e. features).
•	widely used technique because it is very efficient
•	Logistic Regression is also a good baseline that you can use to measure the performance of other more complex Algorithms.
we built the model to logistic regression.
 
We observed the accuracy 77.8 by using the Logistic Regression for classifying the transactions for predicting the fraud transactions.
 
6.	Model Evaluation 
Model	Accuracy in %	False positive	False Negative
KNN	99	7	4
NB	66.7	24	914
Decision Tree	99.5	5	3
Random Forest	99.6	6	0
Logistic Regression	77.6	3	414

After comparing the different models, we found that random regression highest accuracy compared to other models and, same time false negative (Identifying the fraud transaction as non-fraud transaction) is less compared to other models. 
# Conclusion:
Fraud detection often involves a tradeoff between correctly detecting fraudulent samples and not misclassifying many non-fraud samples (Genuine Customer Transactions)
Based on the data and different performance metrics of the different models Decision tree & Random forest Approaches the able to detect with high accuracy and low false Negative rate.
We can create more customer specific models - which are based on customers previous transactional pattern, other details and use them to further improve our decision-making process for identifying the Fraudulent transaction more accurately. 

